type `check_input` <character[] | list<character>> => null;
type `count_characters` <character> => integer;
type `count_sentences` <character> => integer;
type `count_words` <character> => integer;
type `filter` <character[]> => logical[];
type `FUN` <double[]> => double[] | <character[]> => character[] | <double[], double> => logical | <integer, double> => list<double[]> | <character[]> => character;
type `generate_ngrams_batch` <list<character[]>, double, double, character[], character> => list<character[]>;
type `simplify_list` <list<character[]>, logical> => list<character[]>;
type `skip_ngrams_vectorised` <list<character[]>, list<double[]>, character[]> => list<character[]>;
type `tokenize_characters` <character, logical, logical, logical> => list<character[]>;
type `tokenize_characters.default` <character, logical, logical, logical> => list<character[]>;
type `tokenize_character_shingles` <character, double, any, logical, logical, logical> => list<character[]>;
type `tokenize_character_shingles.default` <character, double, double, logical, logical, logical> => list<character[]>;
type `tokenize_lines` <character, logical> => list<character[]>;
type `tokenize_lines.default` <character, logical> => list<character[]>;
type `tokenize_ngrams` <character, logical, double, any, any, character, logical> => list<character[]>;
type `tokenize_ngrams.default` <character, logical, double, double, character[], character, logical> => list<character[]>;
type `tokenize_paragraphs` <character, character, logical> => list<character[]>;
type `tokenize_paragraphs.default` <character, character, logical> => list<character[]>;
type `tokenize_ptb` <character[] | list<character>, logical, logical> => list<character[]>;
type `tokenize_ptb.default` <character[] | list<character>, logical, logical> => list<character[]>;
type `tokenize_sentences` <character, logical, logical, logical> => list<character[]>;
type `tokenize_sentences.default` <character, logical, logical, logical> => list<character[]>;
type `tokenize_skip_ngrams` <character, logical, double, double, double, any, logical> => list<character[]>;
type `tokenize_skip_ngrams.default` <character, logical, double, double, double, character[], logical> => list<character[]>;
type `tokenize_tweets` <character, logical, null, logical, logical, logical> => list<character[]>;
type `tokenize_tweets.default` <character, logical, null, logical, logical, logical> => list<character[]>;
type `tokenize_words` <character, logical, null, logical, logical, logical> => list<character[]>;
type `tokenize_words.default` <character, logical, null, logical, logical, logical> => list<character[]>;
type `tokenize_word_stems` <character, character, null, logical> => list<character[]>;
type `tokenize_word_stems.default` <character, character, null, logical> => list<character[]>;
