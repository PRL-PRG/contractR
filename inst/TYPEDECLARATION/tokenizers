type `check_input` <character[] | list<character>> => null;
type `chunk_individual_text` <character, double, character, ...> => list<character>;
type `chunk_text` <character, double, character, ...> => list<character>;
type `count_characters` <character> => integer;
type `count_sentences` <character> => integer;
type `count_words` <character> => integer;
type `generate_ngrams_batch` <list<character[]>, double, double, character[], character> => list<character[]>;
type `simplify_list` <list<character[]>, logical> => (character[] | list<character[]>);
type `skip_ngrams_vectorised` <list<character[]>, list<double[]>, character[]> => list<character[]>;
type `tokenize_characters` <character, logical, logical, logical> => list<character[]>;
type `tokenize_characters.default` <character, logical, logical, logical> => list<character[]>;
type `tokenize_character_shingles` <character, double, any, logical, logical, logical> => list<character[]>;
type `tokenize_character_shingles.default` <character, double, double, logical, logical, logical> => list<character[]>;
type `tokenize_lines` <character, logical> => list<character[]>;
type `tokenize_lines.default` <character, logical> => list<character[]>;
type `tokenize_ngrams` <character[] | list<character>, logical, double, any, any, character, logical> => list<character[]>;
type `tokenize_ngrams.default` <character[] | list<character>, logical, double, double, character[], character, logical> => list<character[]>;
type `tokenize_paragraphs` <character, character, logical> => list<character[]>;
type `tokenize_paragraphs.default` <character, character, logical> => list<character[]>;
type `tokenize_ptb` <list<character> | character[], logical, logical> => list<character[]>;
type `tokenize_ptb.default` <list<character> | character[], logical, logical> => list<character[]>;
type `tokenize_sentences` <character, logical, logical, logical> => list<character[]>;
type `tokenize_sentences.default` <character, logical, logical, logical> => list<character[]>;
type `tokenize_skip_ngrams` <character, logical, double, double, double, any, logical> => list<character[]>;
type `tokenize_skip_ngrams.default` <character, logical, double, double, double, character[], logical> => list<character[]>;
type `tokenize_tweets` <character, logical, null, logical, logical, logical> => list<character[]>;
type `tokenize_tweets.default` <character, logical, null, logical, logical, logical> => list<character[]>;
type `tokenize_words` <character[] | list<character>, logical, null | character[], logical, logical, logical> => (character[] | list<character[]>);
type `tokenize_words.default` <character[] | list<character>, logical, null | character[], logical, logical, logical> => (character[] | list<character[]>);
type `tokenize_word_stems` <character, character, null, logical> => list<character[]>;
type `tokenize_word_stems.default` <character, character, null, logical> => list<character[]>;
